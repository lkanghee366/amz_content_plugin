"""
Cerebras AI Handler with API Key Rotation
Based on cerebras_client_standalone.py
"""
import os
import time
import logging
from cerebras.cloud.sdk import Cerebras

# Enable DEBUG level logging for this module
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

class CerebrasClient:
    """Cerebras API client with automatic key rotation"""
    
    def __init__(self, api_keys_file: str, model: str = "gpt-oss-120b", cache_file: str = "cerebras_key_cache.txt"):
        """Initialize Cerebras client with API keys from file"""
        self.model = model
        self.cache_file = cache_file
        self.client = None
        self.api_keys = []
        self.key_index = -1  # Initialize key_index
        self.failed_keys = set() # Initialize failed_keys
        self._load_api_keys(api_keys_file)
        self._initialize_client()
        
    def _load_api_keys(self, filepath: str):
        """Load API keys from file"""
        try:
            with open(filepath, 'r') as f:
                self.api_keys = [line.strip() for line in f if line.strip() and not line.startswith('#')]
            if not self.api_keys:
                raise ValueError("No API keys found in file")
            logging.info(f"Loaded {len(self.api_keys)} Cerebras API keys")
        except Exception as e:
            logging.error(f"Failed to load API keys: {e}")
            raise

    def _initialize_client(self):
        """Initialize Cerebras client with a working key"""
        # Try cached key first
        cached_key = self._get_cached_key()
        if cached_key and cached_key in self.api_keys:
            try:
                self.client = Cerebras(api_key=cached_key)
                logging.info(f"Initialized Cerebras client with cached key: ...{cached_key[-4:]}")
                return
            except Exception as e:
                logging.warning(f"Cached key failed: {e}")
        
        # Rotate through keys if no cache or cache failed
        self._rotate_key()

    def _get_cached_key(self) -> str:
        """Read last working key from cache file"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r') as f:
                    return f.read().strip()
            except Exception:
                pass
        return None

    def _save_cached_key(self, key: str):
        """Save working key to cache file"""
        try:
            with open(self.cache_file, 'w') as f:
                f.write(key)
        except Exception as e:
            logging.warning(f"Failed to save key cache: {e}")

    def _rotate_key(self):
        """Find a new working key"""
        for key in self.api_keys:
            try:
                # Test key with a simple client init (SDK doesn't validate until request)
                self.client = Cerebras(api_key=key)
                self._save_cached_key(key)
                logging.info(f"Rotated to new key: ...{key[-4:]}")
                return
            except Exception as e:
                logging.warning(f"Key ...{key[-4:]} failed: {e}")
        
        raise Exception("All Cerebras API keys failed")

    def generate(self, prompt: str, system_prompt: str = None, max_tokens: int = 4000, temperature: float = 0.7) -> str:
        """
        Generate text using Cerebras API
        """
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if not self.client:
                    self._initialize_client()
                    
                response = self.client.chat.completions.create(
                    messages=messages,
                    model=self.model,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content
            except Exception as e:
                logging.warning(f"Cerebras generation failed (attempt {attempt+1}): {e}")
                # If it's an auth error, rotate key
                if "401" in str(e) or "403" in str(e) or "unauthorized" in str(e).lower():
                    logging.info("Auth error detected, rotating key...")
                    self._rotate_key()
                time.sleep(1)
        
        raise Exception("Cerebras generation failed after retries")
        

        # Load API keys
        if not os.path.exists(api_keys_file):
            raise FileNotFoundError(f"API keys file not found: {api_keys_file}")
        
        self.api_keys = []
        with open(api_keys_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.split("#", 1)[0].strip()
                if line:
                    self.api_keys.append(line)
        
        if not self.api_keys:
            raise ValueError(f"No API keys found in {api_keys_file}")
        
        logging.info(f"Loaded {len(self.api_keys)} Cerebras API key(s)")
        
        # Initialize client state
        self.key_index = 0
        self._real_client = None
        self.failed_keys = set()  # Cache failed keys to avoid retrying
        
        # Load cached key index
        cached_index = self._load_key_cache()
        if cached_index is not None:
            logging.info(f"üìÇ Found cached key index: #{cached_index}")
            self._init_client(cached_index)
        else:
            self._init_client(0)
    
    def _load_key_cache(self) -> int:
        """Load cached key index from file"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if content.isdigit():
                        index = int(content)
                        if 0 <= index < len(self.api_keys):
                            return index
                        else:
                            logging.warning(f"‚ö†Ô∏è Cached index {index} out of range, ignoring")
            return None
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Failed to load key cache: {e}")
            return None
    
    def _save_key_cache(self):
        """Save current key index to cache file"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                f.write(str(self.key_index))
            logging.debug(f"üíæ Saved key cache: #{self.key_index}")
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Failed to save key cache: {e}")
    
    def _init_client(self, key_index: int) -> bool:
        """Initialize Cerebras client with specific key index"""
        try:
            key = self.api_keys[key_index]
            # Disable SDK auto-retry to handle retries manually with key rotation
            self._real_client = Cerebras(api_key=key, max_retries=0)
            self.key_index = key_index
            self._save_key_cache()  # Save immediately after successful init
            logging.info(f"‚úÖ Initialized Cerebras client with key #{key_index}")
            return True
        except Exception as e:
            logging.warning(f"‚ö†Ô∏è Failed to initialize with key #{key_index}: {e}")
            return False
    
    def _rotate_key(self) -> bool:
        """Rotate to next API key, skipping failed ones"""
        if len(self.api_keys) <= 1:
            logging.warning("Only 1 API key available, cannot rotate")
            return False
        
        # Find next available key that hasn't failed
        attempts = 0
        while attempts < len(self.api_keys):
            next_index = (self.key_index + 1 + attempts) % len(self.api_keys)
            
            # Skip failed keys
            if next_index in self.failed_keys:
                logging.info(f"‚è≠Ô∏è Skipping key #{next_index} (previously failed)")
                attempts += 1
                continue
            
            if self._init_client(next_index):
                logging.info(f"üîÑ Rotated to API key #{next_index}")
                self._save_key_cache()  # Save after successful rotation
                return True
            else:
                # Mark as failed
                self.failed_keys.add(next_index)
                logging.warning(f"‚ö†Ô∏è Marked key #{next_index} as failed")
                attempts += 1
        
        logging.error("‚ùå No available API keys after rotation (all tried or failed)")
        return False
    
    def generate(self, prompt: str, max_tokens: int = 1024, temperature: float = 0.7, stream: bool = False, use_reasoning: bool = True, model_override: str = None, system_prompt: str = None) -> str:
        """Generate text using Cerebras AI with retry logic
        
        Args:
            prompt: Input prompt
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            stream: Enable streaming response
            use_reasoning: Add reasoning_effort parameter (for gpt-oss-120b)
            model_override: Override default model (e.g., 'llama-3.3-70b' for intro)
            system_prompt: Optional system message to guide model behavior
        """
        attempts = 0
        max_attempts = len(self.api_keys) * 2
        
        # Use override model if specified, otherwise use default
        model_to_use = model_override if model_override else self.model
        
        while attempts < max_attempts:
            try:
                if self._real_client is None:
                    raise RuntimeError("Cerebras client not initialized")
                
                # Build messages array
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                # Build request parameters
                request_params = {
                    "model": model_to_use,
                    "messages": messages,
                    "max_completion_tokens": max_tokens,
                    "temperature": temperature,
                    "stream": stream
                }
                
                # Only add reasoning_effort if use_reasoning is True AND using gpt-oss-120b
                if use_reasoning and model_to_use == "gpt-oss-120b":
                    request_params["reasoning_effort"] = "medium"
                
                response = self._real_client.chat.completions.create(**request_params)
                
                # Handle streaming response
                if stream:
                    content = ""
                    chunk_count = 0
                    for chunk in response:
                        chunk_count += 1
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            if hasattr(delta, 'content') and delta.content:
                                content += delta.content
                    
                    logging.debug(f"üìä Streaming: received {chunk_count} chunks, total {len(content)} chars")
                    
                    if not content:
                        logging.warning("‚ö†Ô∏è Empty content from streaming response")
                    
                    return content
                else:
                    # Non-streaming: get full response
                    logging.debug(f"üìä Response object type: {type(response)}")
                    logging.debug(f"üìä Response choices: {len(response.choices) if hasattr(response, 'choices') else 'N/A'}")
                    
                    if not response.choices or len(response.choices) == 0:
                        logging.error(f"‚ùå No choices in response! Full response: {response}")
                        return ""
                    
                    message = response.choices[0].message
                    content = message.content if hasattr(message, 'content') and message.content else None
                    
                    # gpt-oss-120b sometimes returns content in 'reasoning' field instead of 'content'
                    if not content and hasattr(message, 'reasoning') and message.reasoning:
                        logging.info("üìù Using 'reasoning' field as model returned content there")
                        content = message.reasoning
                    
                    logging.debug(f"üìä Content length: {len(content) if content else 0} chars")
                    
                    if not content:
                        logging.warning(f"‚ö†Ô∏è Empty content from response. Message object: {message}")
                        logging.warning(f"‚ö†Ô∏è Full response object: {response}")
                    else:
                        logging.debug(f"üìù Full content: {content}")
                    
                    # Save cache after successful generation
                    self._save_key_cache()
                    
                    return content if content else ""
                
            except Exception as e:
                err_str = str(e).lower()
                
                # Check for rate limit or auth errors
                if any(code in err_str for code in ["429", "401", "403", "too_many_requests", "unauthorized", "forbidden"]):
                    current_key = self.key_index
                    logging.warning(f"‚ö†Ô∏è API error with key #{current_key}: {e}")
                    
                    # Mark current key as failed
                    self.failed_keys.add(current_key)
                    logging.info(f"üîí Marked key #{current_key} as failed")
                    
                    logging.info(f"üîÑ Attempting key rotation (attempt {attempts + 1}/{max_attempts})")
                    
                    if not self._rotate_key():
                        logging.error("‚ùå All API keys exhausted")
                        raise Exception(f"All Cerebras API keys failed: {e}")
                    
                    # Exponential backoff
                    wait_time = min(2 ** (attempts // len(self.api_keys)), 30)
                    logging.info(f"‚è±Ô∏è Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                    attempts += 1
                    continue
                else:
                    # Non-recoverable error
                    logging.error(f"‚ùå Non-recoverable error: {e}")
                    raise
        
        raise Exception(f"Failed after {max_attempts} attempts")
